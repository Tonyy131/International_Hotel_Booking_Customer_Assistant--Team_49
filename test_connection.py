"""
Quick test script to verify all backend connections are working.
This tests the exact same flow as run_llm_comparison.py
"""
import sys
import os

# Add Graph_RAG to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'Graph_RAG'))

print("=" * 60)
print("TESTING BACKEND CONNECTIONS")
print("=" * 60)

# Test 1: Import modules
print("\n[1/5] Testing module imports...")
try:
    from neo4j_connector import Neo4jConnector
    from retrieval.retrieval_pipeline import RetrievalPipeline
    from llm.llm_answerer import answer_with_model
    from preprocessing.entity_extractor import EntityExtractor
    print("‚úÖ All modules imported successfully")
except Exception as e:
    print(f"‚ùå Import failed: {e}")
    sys.exit(1)

# Test 2: Neo4j connection
print("\n[2/5] Testing Neo4j connection...")
try:
    neo4j_conn = Neo4jConnector()
    print("‚úÖ Neo4j connector initialized")
except Exception as e:
    print(f"‚ö†Ô∏è  Neo4j connection failed: {e}")
    print("   (This is OK if Neo4j is not running)")
    neo4j_conn = None

# Test 3: RetrievalPipeline
print("\n[3/5] Testing RetrievalPipeline...")
try:
    if neo4j_conn:
        rp = RetrievalPipeline(neo4j_conn)
    else:
        rp = RetrievalPipeline()
    print("‚úÖ RetrievalPipeline initialized")
except Exception as e:
    print(f"‚ùå RetrievalPipeline failed: {e}")
    sys.exit(1)

# Test 4: Entity extraction
print("\n[4/5] Testing entity extraction...")
try:
    extractor = EntityExtractor()
    test_entities = extractor.extract("Find hotels in Paris with rating above 8")
    print(f"‚úÖ Entity extraction working: {test_entities}")
except Exception as e:
    print(f"‚ùå Entity extraction failed: {e}")
    sys.exit(1)

# Test 5: Safe retrieval (like in run_llm_comparison.py)
print("\n[5/5] Testing safe_retrieve (main comparison function)...")
try:
    test_query = "Find me hotels in Cairo above 8."
    retrieval = rp.safe_retrieve(
        query=test_query,
        limit=5,
        user_embeddings=True
    )
    
    context_text = retrieval.get("context_text", "")
    hotels = retrieval.get("combined", {}).get("hotels", [])
    
    print(f"‚úÖ safe_retrieve working!")
    print(f"   - Context length: {len(context_text)} chars")
    print(f"   - Hotels found: {len(hotels)}")
    print(f"   - Intent: {retrieval.get('intent', 'N/A')}")
    
    if context_text:
        print(f"\nüìã Sample context:\n{context_text[:200]}...")
    
except Exception as e:
    print(f"‚ùå safe_retrieve failed: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)

# Test 6: Check HF_API_KEY
print("\n[BONUS] Checking HF_API_KEY...")
hf_key = os.getenv("HF_API_KEY")
if hf_key:
    print(f"‚úÖ HF_API_KEY is set (length: {len(hf_key)})")
    print("   Multi-model comparison will use REAL LLM responses")
else:
    print("‚ö†Ô∏è  HF_API_KEY not set")
    print("   Multi-model comparison will fail without it")
    print("   Set it with: $env:HF_API_KEY = 'your_key'")

print("\n" + "=" * 60)
print("CONNECTION TEST SUMMARY")
print("=" * 60)
print("‚úÖ All backend modules: WORKING")
print("‚úÖ RetrievalPipeline: WORKING")
print("‚úÖ Entity extraction: WORKING")
print("‚úÖ safe_retrieve: WORKING")
print(f"{'‚úÖ' if hf_key else '‚ö†Ô∏è '} HF_API_KEY: {'SET' if hf_key else 'NOT SET'}")
print("\nüéâ Your UI is correctly connected to the backend!")
print("   Everything from run_llm_comparison.py is integrated.")
print("\nüí° To test in UI:")
print("   1. Go to http://localhost:8505")
print("   2. Submit any hotel query")
print("   3. Click Tab 6: 'Model Comparison'")
print("   4. Select models and click 'Run Multi-Model Comparison'")
print("=" * 60)
